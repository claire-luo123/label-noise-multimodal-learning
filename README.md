### Multimodal Self-Supervised Learning for Noisy Labels
Performed our research on the Caltech-UCSD Birds dataset (CUB-200-2011 dataset). Designed a multimodal pretext task that uses the image modality to predict the binary attribute modality, which increased model accuracy by 2.48% compared to no pretext task in the presence of label noise.

ABSTRACT:
In current real world applications, deep learning requires large amounts of annotated data to perform well. Manually annotating large datasets is expensive, time consuming, and labor intensive, so researchers often resort to cheaper methods like crowdsourcing. However, these methods also lead to the presence of inaccurate or noisy labels. There has been extensive research focusing on improving robustness to label noise, including data augmentation, loss modeling, semi-supervised learning, and self-supervised learning. To reduce the issue of noisy labels in multimodal datasets, we propose a novel approach of using multimodal self-supervised learning to train deep learning models. In our experiment, we design and use a multimodal self-supervised learning pretext task, where we use the data from one modality to predict the data in another modality. We perform synthetic evaluations on the Caltech-UCSD Birds-200-2011 dataset, and show that our multimodal self-supervised pretext task has a positive effect on network performance in the presence of label noise.

Link to full paper: [https://docs.google.com/document/d/1IdE2-qHc4uLSDBstuJB20-mSbqUSQ4CRWShfl94BT0A/edit?usp=sharing](url)

Acknowledgements:
UCSB Four Eyes Lab, her research group (Mason Wang, Oliver Ni, and Jacob You), Raphael Ruschel, Dr. Lina Kim


